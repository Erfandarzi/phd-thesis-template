
\section{Discussion}
\label{sec:discussion}

% Local training had near random accuracy, but in all the experiments, the performance results were improved. 
% Considering low number of samples for individual clients, a training process limited to few clients results in near random classification accuracy. 
% Local models can have low performance due to limited datasets.
% Also,, ImageNet pre-training had a speed-up effect on our model performance.

% \textbf{Fundamental groups of FL}

% \textbf{how many groups}
% \textbf{how can we group them, name different ways}
% \textbf{effect number of data}


% Paragraph 1: This paragraph provides a “big picture” perspective for readers to remind them of the importance of your study.

% \textbf{overall results}
% 
Our results show that FL has comparable performance to centralized data sharing, with the advantage of keeping data private. With large volumes of data and after high number of rounds, centralized data sharing and cyclic weight transfer have the highest accuracy. 


Sequential models are susceptible to catastrophic forgetting, where a global model performs well on the latest client it has seen while having poor performance in other clients.\quickthings{cryptic sentence}\maybelater{I have paraphrised the sentence}
Conversely, in algorithms like FedAvg and FedSGD, the models are averaged asynchronously after all the clients have finished their training. So the trajectory is smoother and overall improving with more communication rounds. As shown in Figure \ref{seq-vs-noseq}, local test results can have a high variance when passing through clients sequentially, indicating the catastrophic forgetting effect.

Models like FedAvg, and FedSGD, in which all the clients have identical copies of one global model, are slower and more challenging to converge compared to sequential models like CWT and STWT. Also, FedAvG and FedSGD require more training resources due to active server participation, resulting in more computation and network consumption. Stochastic client selection is an efficient way of training. Stochastic models save significant time and resources while having similar performance to full client participation. Overall, CWT and STWT have best results in terms of model accuracy and computation times. These findings could be practical in further federated deployments in medical institutions.
% but when there are communication and computation limitations, other methods might be more practical.




% As can be seen in the results, CWT and STWT have better local results than other methods.

% So each local model is different than other models, and even in fewer rounds they retrain the model on their local data and achieve high accuracies. 

%  Unsurprisingly, CDS has higher accuracy than non-sequential methods.
 
% so in limited number of rounds, latter algorithms are preferred.



%  FedAVG needs to perform both local training and global aggregation, which requires is to have more computation, than sequential models which eliminate global aggregation and broadcasting step.   
%  Models with global server, suh as FedAVG and FedSGD, required more data transfer, because of the two way communication of each client with server.
Sequential models like CWT and STWT perform better than non-sequential models on fewer training rounds. For example,  after three rounds of training, STWT and CWT both reach 96\% accuracy, while FedAvG reaches 66\%, and FedSGD performs equally to a random classifier. As the training proceeds, FedAVG and FedSGD gradually improve with more global rounds.The concept of sequential models is similar to fine-tuning \cite{chen2020online} in centralized deep learning, so in cases where a hospital temporarily joins an FL network, or there is an urgency in training, sequential models are a better option.

More training rounds do not always lead to a better global model. Although average performance on all clients improves, more global rounds lead to worse performance for some clients. The global model can overfit some clients, leading to lower performance on others\cite{mohri2019agnostic}. Some studies suggested early stopping and fine-tuning to local dataset after global training is finished \cite{yu2020salvaging}. 
In all the algorithms, more clients resulted in slower convergence. This effect is stronger in the FedAvg algorithm. In FedAVG, the Global model must compromise between potentially disparate local minima\cite{li2019convergence}. Methods such as adaptive or stochastic selection of clients and momentum-based models help faster convergence \cite{liu2020accelerating}.
Our results suggest that stochastic client participation is close to full client participation. The average results of four trials with varying rounds, shown in Table \ref{number_of_rounds} indicate that stochastic client participation in FedSGD results in 5.23\% performance loss and 40\% less bandwidth consumption compared to FedAvg. In STWT, it results in only 1.25\% less accuracy but saves 40\% of communication and 11.3\% of computation.
% Also, SWT improves local clients' performance up to 80\% accuracy with 8 clients, and it requires extremely low bandwidth requirements.
These results are in accordance with prior studies, showing that, in theory, stochastic and full client participation have similar global minima\cite{cho2020client}. Stochastic client selection can be advantageous when there are limited resources, or in larger networks with occasionally unavailable clients.


% Models like FedSgd and STWT, select the participating clients in a stochastic manner. 
% The reason is because each client has less data, and there will be more number distant local client minima leading to unstable global model.


% Such can be a reflection of real world setting, where not all the clients are active in each round. So a fraction are selected.


% Some models have high max test accuracy while average is low indicating bias and catastrophic forgetting. Infrastructure limitations should be considered, when a central server has limited computing power it would be unable to perform FedAVG, instead no-server algorithms require to pass the model to the next client.

We did not assume any shift in clients' data. A more comprehensive analysis should consider the effect of the domain and distribution shifts on the performance of the algorithms. Also, inter-client data variability and the effect of heterogenous clients could be a future line of research.
% For deployments in longer timeframes, there might be changes in clients data distirbution , and previous results might be inapplicable Effect of domain shifts, and inter-client data variability could be a future line of research. 






\section{Conclusion}
\label{sec:conclusion}
 
FL enables extensive collaborations of hospitals to address medical imaging problems while keeping data private. Real-world implementation requires consideration of efficiency and hardware requirements in addition to model performance, especially in the healthcare field, which generally has limited infrastructure. We implemented five FL algorithms for COVID-19 detection and analyzed their efficiency and accuracy.
Our results suggest that FL algorithms have comparable performance to centralized data sharing, with the advantage of keeping data private. They also show that the sequential methods are a better option in most of the scenarios. This study can be helpful in the deployment of FL systems in COVID-19 detection and medical image analysis in general.

% demonstrate better performance for detecting COVID-19 patients and might be practical in deploying FL algorithms for covid-19 detection and medical image analysis in general.





\printbibliography
