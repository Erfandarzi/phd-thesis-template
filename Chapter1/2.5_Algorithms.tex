
\section{Algorithms}
% \label{sec:algorithms}
\textbf{Centralized Data Sharing} In Centralized Data Sharing (CDS), data is stored in a central location and is accessible to all clients. This stands in contrast to federated and decentralized data sharing methods, where data is stored across multiple locations and accessed by either a single user or a limited number of users. CDS serves as a baseline for comparing other algorithms.
\textbf{Federated Averaging} Federated Averaging involves an iterative learning procedure comprising local and global steps. In this process, each data owner trains a model received from a global server on its local dataset through local iterations \cite{mcmahan2017communication}. Subsequently, the global server aggregates the updated local models to update the global model. This global model is then distributed to clients for the subsequent round. The optimization problem for Federated Averaging can be expressed as:
\begin{equation}
w^{t+1} = \sum\limits_{i=1}^{N}{p_{i} w_{i}^{t}} , w_{i}^{t}=\arg\min\limits_{w_{i}}{\left(\mathcal{L}(\mathcal{D}_{i};w^{t})\right)}
\end{equation}
where $N$ is the number of data owners, $\mathcal{L}(\mathcal{D}_{i};w^{t})$ is a loss function indicating global model parameters  $w^{t}$ of local datasets, and $p_{i}$ is the probability of selecting client $i$. 
Local optimization can be formulated as $w_{i}^{t+1} \leftarrow w^{t}-\eta\cdot \nabla \mathcal{L}(w^{t};\mathcal{D}_{i})$, where 
 $\eta$ is the learning rate. The global model can be updated based on the local models $w_{i}$ and is shared for aggregation: 
\begin{equation}
w^{t+1} = \sum\limits_{i=1}^{N}{p_{i} w_{i}^{t+1}}
\end{equation}

\textbf{Federated Stochastic Gradient Descent} Federated Stochastic Gradient Descent, or FedSGD\cite{chai2020fedeval}, is a variant of Federated Averaging (FedAvg) that employs a large-batch synchronous approach for multi-client learning. In FedSGD, a subset of clients is chosen from the total pool, with this subset being defined by $C$. This selection is made at each global round. Subsequently, the global server dispatches the latest global model to the chosen clients. Each of these clients then undertakes local training on its dataset for a predetermined number of epochs. The global model is updated in light of the local models obtained from each client and is made available for aggregation, a process that is analogous to that in FedAvg. Nevertheless, FedSGD distinguishes itself by computing the gradient over the chosen batch of clients, hence $C < 1$. When $C = 1$, the training becomes non-stochastic, or full batch, as it involves all the clients. This makes it possible to train with large batches since the gradient is computed over the selected subset of clients.

The optimization problem for FedSGD can be expressed as follows:
\begin{equation}
w^{t+1} = w^{t} - \eta \cdot \sum\limits_{i=1}^{C}p_i \cdot \nabla \mathcal{L} (w^t;\mathcal{D}_i)
\end{equation}
where $\eta$ is the learning rate, $p_i$ is the probability of selecting client $i$ and $\mathcal{L}$ is the loss function.
The key difference between FedAvg and FedSGD lies in the use of large-batch synchronous approach in FedSGD. This approach has been shown to outperform the naive asynchronous SGD training due to the increased accuracy and efficiency, as compared to the local training approach used in FedAvg \cite{chai2020fedeval}\cite{charles2021large}. Additionally, FedSGD has been shown to be more robust to non-IID data distributions, compared to FedAvg \cite{chai2020fedeval}. 
\quickthings{This part is unclear to me.}
% \absolutelynecessary{}
\maybelater{paraphrised}

\textbf{Cyclic weight transfer} Federated learning techniques have been widely used in medical image processing tasks using a method known as cyclic weight transfer (CWT)\cite{balachandar2020accounting}. This method involves training models on individual clients for a number of iterations and then cyclically sharing the updated weights with the following client. However, the existing CWT algorithm faces a notable challenge, as it lacks the ability to effectively manage inter-client variability in training data or labels.  To ensure the practical application of CWT, it is crucial to develop a version that can handle the common variations observed in a majority of real-world medical imaging datasets\cite{darzidehkalanifederatedII}.

\textbf{Single weight transfer} Single weight transfer (SWT) is anothe FL method widely used in the medical imaging domain. In Single weight transfer, models are trained in each client with its local data, and then the updated model is transferred to the next client. The difference between this method and CWT is that here the model passes each client only once\cite{darzidehkalanifederatedI}. 

\textbf{Stochastic weight transfer}
In stochastic weight transfer (STWT), we select a subsample of clients and train them in a cyclic manner. Similar to FedSGD, a ratio defines the number of selected clients to the total number of clients in each federated round. 

Figure \ref{fig:flalgorithms} provides a visual representation of the introduced algorithms.



% سوال / مسیله چیه
% Importance: Why your research matters in the context of an industry or the world
% اهمیت موضوع
% 	کجاها کاربرد داره
% مشکلات موضوعات قبلی و اینکه مسایل دگ کار نمیکنن
% برای اینکه این مسایل کار کنن نیازی به چه چیزی بود. ک اونا کم داشتن
% چرا این روش تو داره کار میکنه
% چه سوالی رو جواب میده
% توضیح اینکه چجوری کار میکنه
% تعریف و تمجید از کار کردنش

% \maybelater{chanta akse tamiz ba coreldraw bekesh}

% Their
% We are comparing various implementations of FL and comparing their performance level. The performance is done by metrics.
% 2. Although FL has been introduced to tackle the problem of privacy, its distributed nature requires too much commuincation between clients. Knowsing can determine its deployability and scailibitlty. Hence the question is how to these model compare it terms of communication between clients.
% 3. Computation time. Each FL model is investigated based on the required time for computation.
% 4. Effect of rounds: Number of Federated rounds 


% \quickthings{Maybe you can add a benchmarking section similar to "A Performance Evaluation of FL Algorithms"}
% \maybelater{Some ideas are herer https://arxiv.org/pdf/1709.05929.pdf
% }

